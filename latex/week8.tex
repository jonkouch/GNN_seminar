\documentclass{beamer}

% Theme choice (you can change this to your preferred theme)
\usetheme{Madrid}
\usepackage{bm}

% Title page information
\title[Equivariant Graph Neural Networks]{Equivariant Graph Neural Networks}
% 3 authors
\author[Kfir Eliyahu, Ben Eliav, Jonathan Kouchly]{Kfir Eliyahu \and Ben Eliav \and Jonathan Kouchly}

\date{\today} % Use specific date if needed

\newcommand{\FrameNumberDoesNotAdvance}{
\setbeamertemplate{footline}{}
\addtocounter{framenumber}{-5}}

% Define a toggle for presentation mode
\newif\ifpresentation
\presentationfalse % Set to \presentationtrue for presentation mode, \presentationfalse for final version

\AtBeginSection[]
{
{%\FrameNumberDoesNotAdvance{}
\begin{frame}
\tableofcontents[
currentsection,
subsectionstyle=show/shaded
]
\end{frame}
}}

\AtBeginSubsection[]
{
{%\FrameNumberDoesNotAdvance{}

\begin{frame}
\tableofcontents[
currentsection,
currentsubsection
]
\end{frame}
}}


\begin{document}

% Title Page
\begin{frame}
    \titlepage
\end{frame}

% Table of Contents (Optional)
\begin{frame}{Outline}
    \tableofcontents
\end{frame}

% Sections
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Motivation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Motivation}
\begin{itemize}
    \setlength{\itemsep}{\fill}
    \item Our neural networks can operate on data of many types.
    %\pause
    \item We often work with images, text, audio, graphs and more.
    %\pause
    \item These data types have different structures and qualities, and we would like to build architectures that best suit them.
    \end{itemize}
    %\pause
    \begin{center}
        \includegraphics[width=0.5\textwidth]{../figures/equivariance.png}
    \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Motivation}
    \begin{itemize}
        \setlength{\itemsep}{\fill}
        \item A cat is a cat no matter how you look at it.
    \end{itemize}

    \begin{center}
        \includegraphics[width=0.8\textwidth]{../figures/cat.png}
    \end{center}

    \begin{itemize}
        \setlength{\itemsep}{\fill}
        %\pause
        \item It is acceptable to assume that being invariant to the rotation of the cat is a good property for a classification network.
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Motivation}
    \begin{itemize}
        \setlength{\itemsep}{\fill}
        \item Our focus today is on sets and graph data.
        \end{itemize}
    \begin{center}
        \begin{minipage}[t]{0.69\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../figures/graph_adj.png}
        \end{minipage}
        \hfill
        \begin{minipage}[t]{0.3\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../figures/set.png}
        \end{minipage}
    \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Construction of an Equivariant Neural Network}

    \begin{itemize}
        \setlength{\itemsep}{\fill}
        \item When contructing an equivariant neural network, two things should always be considered:
        \begin{enumerate}
            %\pause
            \item The symmetries of the data:\\
                What inherent structure should our model be oblivious to?
            %\pause
            \item The space of functions learnable by the network:\\ 
                Are we fully utilizing the space of functions that are equivariant 
        \end{enumerate} 
    \end{itemize}
    
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Mathematical Background}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{The Permutation Group $S_n$}

    \begin{itemize}
        \setlength{\itemsep}{\fill}
        \item The permutation group $S_n$ is the group of all permutations of $n$ elements.
        \item It has $n!$ elements, representing the $n!$ ways to order $n$ elements.
        \item Given a set $X = \{x_1, x_2, \ldots, x_n\}$, a permutation $\pi \in S_n$ is a bijection $\pi: X \rightarrow X$
        \item e.g. $x = (x_1, x_2, x_3)$, and $\pi = (1, 2, 3) \in S_3$ is the permutation that maps $1 \rightarrow 2$, $2 \rightarrow 3$ and $3 \rightarrow 1$.
        \item We denote the \textbf{action} of $\pi$ on $x$ as $\pi x = (x_3, x_1, x_2)$. 
    \end{itemize}

    
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Permutation Invariance}

    \begin{itemize}
        \setlength{\itemsep}{\fill}
        \item Let $H \leq S_n$ be a subgroup of the symmetric group.
        %\pause
        \item $f:\mathbb{R}^n \rightarrow \mathbb{R}$ is \textit{permutation invariant} if $f(x) = f(\pi x)$ for all $\pi \in H$.
    \end{itemize}
    \begin{center}
        %\pause
        \includegraphics[width=0.65\textwidth]{../figures/perm_in.png}
    \end{center}
    
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Permutation Equivariance}

    \begin{itemize}
        \setlength{\itemsep}{\fill}
        \item Let $H \leq S_n$ be a subgroup of the symmetric group.
        %\pause
        \item $f:\mathbb{R}^n \rightarrow \mathbb{R}^n$ is \textit{permutation equivariant} if $\pi f(x) = f(\pi x)$ for all $\pi \in H$.
    \end{itemize}
    \begin{center}
        %\pause
        \includegraphics[width=0.65\textwidth]{../figures/perm_eq.png}
    \end{center}
    
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Permutation of a Set}

    \begin{itemize}
        \setlength{\itemsep}{\fill}
        \item Assume our set is $X = \{x_1, x_2, \ldots, x_n\}$.
        %\pause
        \item We can represent $X$ as a matrix $X \in \mathbb{R}^{n \times d}$.
        %\pause
        \item Any permutation $g \in S_n$ can be represented as a permutation matrix $\boldsymbol{P} \in \mathbb{R}^{n \times n}$,
        %\pause
        \item The action of $g$ on $X$ is then $P X$.
        %\pause
        \item An invariant neural network is a function $f: \mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{d`}$ such that $f(X) = f(\boldsymbol{P}X)$.
        %\pause
        \item An equivariant neural network is a function $f: \mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{n \times d`}$ such that $\boldsymbol{P}f(X) = f(\boldsymbol{P}X)$.
    \end{itemize}
    
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Permutation of a Graph}

    \begin{itemize}
        \setlength{\itemsep}{\fill}
        \item Our data is now a graph adjacency matric $A \in \mathbb{R}^{n \times n}$. 
        %\pause
        \item A permutation matrix $\boldsymbol{P} \in \mathbb{R}^{n \times n}$ acts on the adjacency matrix $A$.
        %\pause
        \item The action of $\boldsymbol{P}$ on $A$ is:
        \[ \boldsymbol{P^T}A\boldsymbol{P} \]
    \end{itemize}
    
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Equivariant Network Construction}

    \begin{theorem}
        Let $L$ be a linear equivariant layer, and let $f$ be a neural network constructed be stacking $L$ and non-linearities $\sigma$. Then $f$ is permutation equivariant.
    \end{theorem}
    %\pause
    \begin{proof}
        Let $x$ be a set of $n$ elements, and let $g \in S_n$ be a permutation.
        %\pause
        \[ f(gx) = L(\sigma(L(\sigma(\ldots L(gx) \ldots)))) = L(\sigma(L( \ldots g\sigma(L(x)) \ldots))) = \ldots \]
        %\pause
        \[ g L(\sigma(L(\sigma(\ldots L(x) \ldots)))) = gf(x) \]
    \end{proof}
    
    
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Invariant Network Construction}
    \begin{theorem}
        Let $f$ be an equvariant neural network and let $\phi$ be a permutation invariant function. Then $h = \phi(f(x))$ is a permutation invariant neural network.
    \end{theorem}
    %\pause
    \begin{proof}
        Let $x$ be a set of $n$ elements, and let $g \in S_n$ be a permutation.
        %\pause
        \[ h(gx) = \phi(f(gx)) = \phi(gf(x)) = \phi(f(x)) = h(x) \]
    \end{proof}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Deep Sets}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Deep Sets}

    \begin{itemize}
        \setlength{\itemsep}{\fill}
        \item A seminal work in the field of equivariant neural networks. 
        %\pause
        \item Recall the two properties we mentioned earlier (symmetries of the data and the space of functions learnable by the network).
        %\pause
        \item \emph{DeepSets} is an architecture that is equivariant to set permutations and is maximmally expressive in the space of permutation equivariant functions.
        %\pause
        \item We are going to see the construction and prove it satisfies equivariance and expressiveness.
    \end{itemize}
    
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\emph{DeepSets} Layer}

    \begin{itemize}
        \setlength{\itemsep}{\fill}
        \item We saw a general structure of an invariant and equivariant network.
        \item To fill in the details, we need to define the equivariant layer $L$ and the invariant function $\phi$.
    \end{itemize}

    %\pause
    \begin{definition}
        Consider a set $x = \{x_1, x_2, \ldots, x_n\}$, where $x_i \in \mathbb{R}$.\\
        A \emph{DeepSets} layer is defined as 
        \[L(x) = \lambda\mathrm{I}\mathbf{x} + \mathbf{x}\mathbf{1}\].
    \end{definition}
    
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\emph{DeepSets} Layer}

    \begin{definition}
        Consider a set $x = \{x_1, x_2, \ldots, x_n\}$, where $x_i \in \mathbb{R}$.\\
        A \emph{DeepSets} layer is defined as 
        \[L(x) = \lambda\mathrm{I}\mathbf{x} + \mathbf{x}\mathbf{1}\].
    \end{definition}

    %\pause
    \begin{theorem}
        A \emph{DeepSets} layer is permutation equivariant.
    \end{theorem}

    %\pause
    \begin{proof}
        Let $x$ be a set of $n$ elements, and let $g \in S_n$ be a permutation.
        %\pause
        \[ L(gx) = \lambda\mathrm{I}(gx) + (gx)\mathbf{1} = g(\lambda\mathrm{I}x) + x\mathbf{1} = gL(x)\]
    \end{proof}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\emph{DeepSets} Network}
    \begin{itemize}
        \setlength{\itemsep}{\fill}
        \item We have an initial layer $L$, which we proved is equivariant.
        %\pause
        \item A deep sets invariant network is now constructed as:\\
        \[f(x) = \phi(L\sigma(L\sigma(\ldots L\sigma(L(x)) \ldots))) \quad \text{where} \quad \phi(x) = \sum_{i=1}^{n}x_i \]
        %\pause
        \item It is easy to see that $\phi$ is permutation invariant, and thus $f$ is permutation invariant.
        \item For a classification network, take some classification module $\rho$ (e.g. an MLP), and define the final network as:
        \[ h(x) = \rho(f(x)) \]  
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\emph{DeepSets} Network}
    \begin{itemize}
        \setlength{\itemsep}{\fill}
        \item Notice that the network is only defined for sets with elements in $\mathbb{R}$.
        \item We can extend this to a set $X\in\mathbb{R}^{n \times d}$ by defining $L$ as:
        \[L(x) = \mathbf{X}W_1 + \mathbf{1}\mathbf{1}^T\mathbf{X}W_2\]
        \item This keeps the general structure of the layer: a linear transformation of the distinct elements of the set summed with the mean of the set.  
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\emph{DeepSets} Expressivity}
    \begin{itemize}
        \setlength{\itemsep}{\fill}
        \item We have shown that the \emph{DeepSets} network is permutation invariant.
        %\pause
        \item We now want to show that it is maximally expressive in the space of permutation invariant functions.
        %\pause
        \item We show outline for the proof, showing \emph{DeepSets} can separate any two sets that are not equal.

    \end{itemize}
    %\pause
    \begin{theorem}
        Let $x,y \in \mathbb{R}^{n \times d} \quad$ s.t $\quad x \neq gy \quad \forall g \in S_n$. Then there exists a \emph{DeepSets} network that separates $x$ and $y$, namely $F(x ; \theta) \neq F(y ; \theta)$. 
    \end{theorem}
    
    

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\emph{DeepSets} Expressivity}
    Proof outline:
    \begin{enumerate}
        \setlength{\itemsep}{\fill}
        %\pause
        \item Consider distinct rows of $x, y$.
        %\pause
        \item Construct input output pairs using standard basis elements (one hot encodings of the elements in the set).
        \item In each layer $L(x) = \mathbf{X}W_1 + \mathbf{1}\mathbf{1}^T\mathbf{X}W_2$, set $W_2 = 0$, collapsing our network to a standard MLP.
        %\pause
        \item According to a result of the universal approximation theorem, we can learn a one to one mapping between the input and output pairs.
        %\pause
        \item Output for each set the sum of the output pairs.
        %\pause
        \item For each set, we computed the  hitogram of its elements, which is a unique representation of the set.
    \end{enumerate}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Invariant and Equivariant Graph Networks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Invariant and Equivariant Graph Networks}
    \begin{itemize}
        \setlength{\itemsep}{\fill}
        \item We saw a simple, maximally expressive equivariant network for sets.
        %\pause
        \item We now want to extend this to graphs.
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Invariant and Equivariant Graph Networks}
    \begin{itemize}
        \setlength{\itemsep}{\fill}
        \item Recall the action of the permutation matrix $\boldsymbol{P}$ on the adjacency matrix $A$ is:
        \[\boldsymbol{P^T}A\boldsymbol{P}\]
        %\pause
        \item Lets use this to define a Linear invariant layer $L \in \mathbb{R}^{n \times n}$. We want to satisfy the following:
        \[ L\boldsymbol{P^T}A\boldsymbol{P} = L(A) \]
        %\pause
        \item But working with bilinear forms is hard. We can define an equivalent condition by column stacking the adjacency matrix to get a vector, and then define a linear layer $L \in \mathbb{R}^{1 \times n^2}$.
        \[ L\text{vec}(\boldsymbol{P}^T A\boldsymbol{P}) = L\text{vec}(A) \]
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Invariant and Equivariant Graph Networks}
    \begin{itemize}
        \setlength{\itemsep}{\fill}
        \item We now introduce a crucial property of the kronecker product:
        \[ \text{vec}(XAY)= Y^T\otimes X\text{vec}(A)\]
        %\pause
        \item Using this, the previous condition can be written as:
        \[ L(P^T\otimes P^T)\text{vec}(A) = L\text{vec}(A) \]
        %\pause
        \item For this condition to hold for all $A$, we need $L$ to satisfy:
        \[ L(P^T\otimes P^T) = L \]
        %\pause
        \item transposing the equation, and with severe abuse of notation $(L^T = \text{vec(L)})$, we finally get:
        \[ (P\otimes P)\text{vec}(L) = \text{vec}(L) \]
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Invariant and Equivariant Graph Networks}
    \begin{itemize}
        \setlength{\itemsep}{\fill}
        \item After some work and moderate logic jumps, we got a condition for a permutation invariant linear layer $L$.
        %\pause
        \item Developing the condition for an equivariant layer is very similar:
        %\pause
        \[ L\text{vec}(\boldsymbol{P}^T A\boldsymbol{P}) =\boldsymbol{P}^T L\text{vec}(A) \boldsymbol{P}\]
        %\pause
        \item Using the kronecker product property, we get:
        \[ L(P^T\otimes P^T)\text{vec}(A) = (P^T\otimes P^T)L\text{vec}(A) \]
        %\pause
        \item This should hold for every $A$, so we get:
        \[ L(P^T\otimes P^T) = (P^T\otimes P^T)L \]
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Invariant and Equivariant Graph Networks}
    \begin{itemize}
        \setlength{\itemsep}{\fill}
        \item $(P^T\otimes P^T)$ is an $n^2 \times n^2$ premutation matrix, and its inverse is $(P\otimes P)$.
        %\pause
        \[ (P^T\otimes P^T)L(P^T\otimes P^T) = L \]
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Conclusion}
    \begin{itemize}
        \item end
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% Thank You Slide
\begin{frame}[plain]
    \centering
    \Huge Thank You!
\end{frame}

\end{document}
